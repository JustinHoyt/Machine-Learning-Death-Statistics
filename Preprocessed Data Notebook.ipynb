{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd                 # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt      # matplotlib.pyplot plots data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./DeathRecords/new_data.csv\")\n",
    "del df['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = df.columns.values\n",
    "\n",
    "x = df[feature_col_names].values     # predictor feature columns (8 X m)\n",
    "split_test_size = 0.30\n",
    "\n",
    "X_train, X_test = train_test_split(x, test_size=split_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class CategoricalNB(sklearn.naive_bayes.BaseDiscreteNB):\n",
    "    \"\"\"Naive Bayes classifier for multivariate Categorical models.\n",
    "    Like MultinomialNB, this classifier is suitable for discrete data. The\n",
    "    difference is that while MultinomialNB works with occurrence counts,\n",
    "    CategoricalNB is designed for categorical features.\n",
    "    Read more in the :ref:`User Guide <categorical_naive_bayes>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional (default=1.0)\n",
    "        Additive (Laplace/Lidstone) smoothing parameter\n",
    "        (0 for no smoothing).\n",
    "    fit_prior : boolean, optional (default=True)\n",
    "        Whether to learn class prior probabilities or not.\n",
    "        If false, a uniform prior will be used.\n",
    "    class_prior : array-like, size=[n_classes,], optional (default=None)\n",
    "        Prior probabilities of the classes. If specified the priors are not\n",
    "        adjusted according to the data.\n",
    "    Attributes\n",
    "    ----------\n",
    "    class_log_prior_ : array, shape = [n_classes]\n",
    "        Log probability of each class (smoothed).\n",
    "    feature_log_prob_ : array of dictionaries, shape = [n_classes, n_features, n_feature_vals]\n",
    "        Empirical log probability of features given a class, P(x_i|y).\n",
    "    class_count_ : array, shape = [n_classes]\n",
    "        Number of samples encountered for each class during fitting. This\n",
    "        value is weighted by the sample weight when provided.\n",
    "    feature_count_ : array, shape = [n_classes, n_features, num_feature_vals]\n",
    "        Number of samples encountered for each (class, feature, feature val)\n",
    "        during fitting. This value is weighted by the sample weight when\n",
    "        provided.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.random.randint(2, size=(6, 100))\n",
    "    >>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
    "    >>> from sklearn.naive_bayes import CategoricalNB\n",
    "    >>> clf = CategoricalNB()\n",
    "    >>> clf.fit(X, Y)\n",
    "    CategoricalNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "    >>> print(clf.predict(X[2:3]))\n",
    "    [3]\n",
    "    References\n",
    "    ----------\n",
    "    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "    Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
    "    A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
    "    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
    "    Text Categorization, pp. 41-48.\n",
    "    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
    "    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, binarize=.0, fit_prior=True,\n",
    "                 class_prior=None,feature_space=None,output_space=None,n_classes=0,max_EM_iter=50):\n",
    "        self.alpha = alpha\n",
    "        self.binarize = binarize\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "\n",
    "        if class_prior is None:\n",
    "            self.class_log_prior_ = None\n",
    "        else:\n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "\n",
    "        self.output_space = output_space\n",
    "        self.map_output_space = {}\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.class_count_ = None\n",
    "        self.feature_count_ = None\n",
    "\n",
    "        if n_classes > 0:\n",
    "            self._reset_class_count()\n",
    "            \n",
    "        if output_space is not None:\n",
    "            self.init_output_space()\n",
    "        self.feature_space = feature_space\n",
    "\n",
    "        if feature_space is None:\n",
    "            self.n_feature = None\n",
    "            self.n_feature_vals = None\n",
    "            self.map_feature_space = None\n",
    "        else:\n",
    "            self.n_features = len(feature_space)\n",
    "            self.set_n_feature_vals()\n",
    "            self.set_map_feature_space()\n",
    "            self._reset_feature_count()\n",
    "\n",
    "        self.delta_ = None\n",
    "        self.max_EM_iter = max_EM_iter\n",
    "        self.feature_log_prob_ = None\n",
    "\n",
    "\n",
    "    def set_map_feature_space(self):\n",
    "        self.map_feature_space = []\n",
    "        for i in range(self.n_features):\n",
    "            self.map_feature_space.append({})\n",
    "            for j,v in zip(range(self.n_feature_vals[i]),self.feature_space[i]):\n",
    "                self.map_feature_space[i][str(v)] = j\n",
    "            \n",
    "    def set_n_feature_vals(self):\n",
    "        self.n_feature_vals = []\n",
    "        for i in range(self.n_features):\n",
    "            self.n_feature_vals.append(len(self.feature_space[i]))\n",
    "                \n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        self.class_count_ += self._expected_class_count(Y)\n",
    "        tmp_feature_count_ = self._expected_class_feature_count(X,Y)\n",
    "        for k in range(self.n_classes):\n",
    "            for i in range(self.n_features):\n",
    "                self.feature_count_[k,i] += tmp_feature_count_[k,i]\n",
    "        #self.feature_count_ += self._expected_class_feature_count(X,Y)\n",
    "\n",
    "    def _M_step(self):\n",
    "        self._update_feature_log_prob()\n",
    "        self._update_class_log_prior(class_prior=self.class_prior)\n",
    "\n",
    "    def _E_step(self,X,Y):\n",
    "        m = self.n_classes\n",
    "        num_samples = X.shape[0]\n",
    "        self.delta_ = self.predict_proba(X)\n",
    "        #print self.delta_.shape\n",
    "        for l in range(num_samples):\n",
    "            if Y[l] is not None:\n",
    "                self.delta_[l,:] = np.zeros(self.n_classes)\n",
    "                self.delta_[l,self.map_output_space[str(Y[l])]] = 1.0\n",
    "        self._count(X, Y)\n",
    "    \n",
    "    def _expected_class_feature_count(self, X, Y):\n",
    "        m = self.n_classes\n",
    "        num_samples = X.shape[0]\n",
    "        n = self.n_features\n",
    "        count = np.empty((m, n),dtype=object)\n",
    "        for c in range(m):\n",
    "            for i in range(n):\n",
    "                count[c,i] = np.zeros(self.n_feature_vals[i])\n",
    "                \n",
    "        for l in range(num_samples):\n",
    "            if Y[l] is not None:\n",
    "                for i in range(n):\n",
    "                    count[self.map_output_space[str(Y[l])],i][self.map_feature_space[i][str(X[l,i])]] += 1.0\n",
    "            else:\n",
    "                for c in range(m):\n",
    "                    for i in range(n):\n",
    "                        count[c,i][self.map_feature_space[i][str(X[l,i])]] += self.delta_[l,c]\n",
    "        return count\n",
    "\n",
    "    def _expected_class_count(self, Y):\n",
    "        m = self.n_classes\n",
    "        count = np.zeros(m)\n",
    "        n_samples = self.delta_.shape[0]\n",
    "        for l in range(n_samples):\n",
    "            if Y[l] is not None:\n",
    "                count[self.map_output_space[str(Y[l])]] += 1.0\n",
    "            else:\n",
    "                count += self.delta_[l,:]                \n",
    "        return count\n",
    "\n",
    "    def _update_feature_log_prob(self):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_\n",
    "        for k in range(self.n_classes):\n",
    "            for i in range(self.n_features):\n",
    "                smoothed_fc[k,i] += self.alpha\n",
    "                self.feature_log_prob_[k,i] = (np.log(smoothed_fc[k,i]) - np.log(np.sum(smoothed_fc[k,i])))\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        # check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        print(\"_joint_log_likelihood...\")\n",
    "        \n",
    "        n_classes, n_features = self.feature_log_prob_.shape\n",
    "        n_samples, n_features_X = X.shape\n",
    "\n",
    "        if n_features_X != n_features:\n",
    "            raise ValueError(\"Expected input with %d features, got %d instead\"\n",
    "                             % (n_features, n_features_X))\n",
    "\n",
    "        jll = np.zeros((n_samples,n_classes))\n",
    "        for l in range(n_samples):\n",
    "            for c in range(n_classes):\n",
    "                for i in range(n_features):\n",
    "                    jll[l,c] += self.feature_log_prob_[c,i][self.map_feature_space[i][str(X[l,i])]]\n",
    "            jll[l,:] += self.class_log_prior_\n",
    "\n",
    "        print(\"Avg. log. likelihood: %f\" % (np.mean(logsumexp(jll,axis=1))))\n",
    "        print(\"done\")\n",
    "        return jll\n",
    "\n",
    "    def set_feature_space(self, X):\n",
    "        self.n_features = X.shape[1]\n",
    "        self.feature_space = []\n",
    "        for i in range(self.n_features):\n",
    "            self.feature_space.append(list({str(v) for v in frozenset(X[:,i])}))\n",
    "        self.set_n_feature_vals()\n",
    "        self.set_map_feature_space()\n",
    "        self._reset_feature_count()\n",
    "\n",
    "    def init_output_space(self):\n",
    "        self.n_classes = len(self.output_space)\n",
    "        self.map_output_space = {}\n",
    "        for k,s in zip(range(self.n_classes),[str(v) for v in self.output_space]):\n",
    "            self.map_output_space[s] = k\n",
    "            \n",
    "    def set_output_space(self, y):\n",
    "        S = {str(c) for c in y.unique()}\n",
    "        self.output_space = list(S)\n",
    "        self.init_output_space()\n",
    "\n",
    "    def _rand_unif_dist(self, n_vals):\n",
    "        vals = np.zeros(n_vals+1)\n",
    "        vals[n_vals] = 1.0\n",
    "        vals[1:n_vals] = np.sort(np.random.rand(n_vals-1))\n",
    "        return np.diff(vals)\n",
    "\n",
    "    def rand_init_class_log_prior(self):\n",
    "        self.class_log_prior_ = np.log(self._rand_unif_dist(self.n_classes))\n",
    "\n",
    "    def rand_init_feature_prob(self):\n",
    "        self.feature_log_prob_ = np.empty((self.n_classes, self.n_features),dtype=object)\n",
    "        for c in range(self.n_classes):\n",
    "            for i in range(self.n_features):\n",
    "                self.feature_log_prob_[c,i] = np.log(self._rand_unif_dist(self.n_feature_vals[i]))\n",
    "\n",
    "    def _reset_count(self):\n",
    "        self._reset_class_count()\n",
    "        self._reset_feature_count()\n",
    "\n",
    "    def _reset_feature_count(self):\n",
    "        self.feature_count_ = np.empty((self.n_classes, self.n_features),dtype=object)\n",
    "        for k in range(self.n_classes):\n",
    "            for i in range(self.n_features):\n",
    "                self.feature_count_[k,i] = np.zeros(self.n_feature_vals[i])\n",
    "\n",
    "    def _reset_class_count(self):\n",
    "        self.class_count_ = np.zeros(self.n_classes)\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Naive Bayes classifier according to X, y\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.feature_space is None:\n",
    "            self.set_feature_space(X)\n",
    "\n",
    "        if self.output_space is None:\n",
    "            self.set_output_space(y)\n",
    "\n",
    "        self.classes_ = np.array(self.output_space)\n",
    "        \n",
    "        if self.class_log_prior_ is None:\n",
    "            self.rand_init_class_log_prior()\n",
    "\n",
    "        if self.feature_log_prob_ is None:\n",
    "            self.rand_init_feature_prob()\n",
    "        \n",
    "        # Run Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "        print(np.exp(self.class_log_prior_))\n",
    "        \n",
    "        for t in range(self.max_EM_iter):\n",
    "            #print t\n",
    "            self._E_step(X,y)\n",
    "            self._M_step()\n",
    "            print(np.exp(self.class_log_prior_))\n",
    "            self._reset_count()\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd                 # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt      # matplotlib.pyplot plots data\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# Read in data\n",
    "\n",
    "df = pd.read_csv(\"./DeathRecords/new_data.csv\")\n",
    "del df['Id']\n",
    "\n",
    "# Split into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = df.columns.values\n",
    "\n",
    "x = df[feature_col_names].values     # predictor feature columns (8 X m)\n",
    "split_test_size = 0.30\n",
    "\n",
    "X_train, X_test = train_test_split(x, test_size=split_test_size, random_state=42)\n",
    "\n",
    "X_train_cv, X_test_cv = train_test_split(X_train, test_size=split_test_size)\n",
    "\n",
    "# Categorical Bernoulli Naive Bayes\n",
    "\n",
    "# from categorical_nb import CategoricalNB\n",
    "\n",
    "Y_train_cv = []\n",
    "#for row in range(len(df)):\n",
    "for l in range(X_train_cv.shape[0]):\n",
    "    Y_train_cv.append(None)\n",
    "\n",
    "#print X_train.shape\n",
    "# s = X_train[:,1]\n",
    "\n",
    "# print s, len(s)\n",
    "\n",
    "# S = frozenset(s)\n",
    "\n",
    "# print S, len(S)\n",
    "\n",
    "# S_asstr = {str(v) for v in S}\n",
    "\n",
    "# print S_asstr, len(S_asstr)\n",
    "\n",
    "# S_aslist = list(S_asstr)\n",
    "\n",
    "# print S_aslist, len(S_aslist)\n",
    "\n",
    "#print [str(v) for v in range(3)]\n",
    "    \n",
    "hold_out_avg_log_likelihood = []    \n",
    "\n",
    "# try doing the binary search yourself\n",
    "# k = number of clusters\n",
    "for k in np.arange(10,41,10):\n",
    "    # n_classes = num clusters, output_space = assigning the cluster a label\n",
    "    nb_model = CategoricalNB(n_classes=k,output_space=[str(v) for v in range(k)],max_EM_iter=1)\n",
    "    \n",
    "    nb_model.set_feature_space(x)\n",
    "\n",
    "    # iter = num of iterations\n",
    "\n",
    "    prev_cv_avg_log_likelihood = float('-inf')\n",
    "    for iter in np.arange(1,10000000):\n",
    "        nb_model.fit(X_train_cv[0:10000,:], y=Y_train_cv[0:10000])\n",
    "        \n",
    "        cv_jll = nb_model._joint_log_likelihood(X_test_cv[0:10000,:])\n",
    "        cv_avg_log_likelihood = np.mean(logsumexp(cv_jll,axis=1))\n",
    "        result = [k, iter, cv_avg_log_likelihood]\n",
    "        hold_out_avg_log_likelihood.append(result)\n",
    "        print(result)\n",
    "        if cv_avg_log_likelihood > prev_cv_avg_log_likelihood:\n",
    "            prev_cv_avg_log_likelihood = cv_avg_log_likelihood\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "# print(hold_out_avg_log_likelyhood)\n",
    "# find best k and iters from hold_out_log_likelihood (3rd element in lisr)\n",
    "\n",
    "#hold_out_log_likelihood = np.matrix(hold_out_avg_log_likelyhood, dtype=np.float64)\n",
    "#k_best, iter_best, cv_avg_log_likelihood_best = hold_out_log_likelihood_matrix[np.argmax(hold_out_log_likelihood_matrix[:,2]),:]\n",
    "k_best, iter_best, cv_avg_log_likelihood_best = hold_out_avg_log_likelihood[np.argmax(hold_out_avg_log_likelihood[:][2])][:]\n",
    "\n",
    "nb_model_best = CategoricalNB(n_classes=k_best,output_space=[str(v) for v in range(k_best)],max_EM_iter=iter_best)\n",
    "nb_model_best.set_feature_space(x)\n",
    "\n",
    "nb_model_best.fit(X_train, y=Y_train)\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump('~/Documents/best_nb_model.pkl',nb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2631171\n"
     ]
    }
   ],
   "source": [
    "feature_values = []\n",
    "for feature in df.columns:\n",
    "    feature_values.append(df[feature].unique())\n",
    "\n",
    "Y = []\n",
    "for row in range(len(df)):\n",
    "    Y.append(None)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
